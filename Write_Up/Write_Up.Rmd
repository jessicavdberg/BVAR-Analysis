---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Forecasting Model for South Africa's Economic Growth: A Bayesian Vector Autoregresssion Approach"
#subtitle: "no"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Jessica Van der Berg"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stelenbosch University, South Africa" # First Author's Affiliation
Email1: "20190565\\@sun.ac.za" # First Author's Email address


#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
keywords: "Forecasting \\sep Bayesian \\sep VAR" # Use \\sep to separate
JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

Adding additional latex packages:
# header-includes:
    - \usepackage{mathtools} # Add additional packages here.
    - \usepackage{graphicx}
    - \usepackage{amsmath}
    - \usepackage{amssymb}
    - \usepackage{multirow}

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  Forecasting macroeconomic variables are critical for developing policies. This paper sets out to forecast real GDP growth in South Africa by using a Bayesian vector autoregressive (BVAR) using data from 1980:Q3 to 2009:Q2. BVAR models are used to avoid problems of multicollinearity and over parameterization that occur with the classical vector autoregression models. The results from the model confirm the accuracy of BVAR models for forecasting key macroeconomic variables, such as economic growth
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
#library(tidyverse)
#library(readxl)
#library(ggplot2)
#library(Rcpp)
#library(tseries)
set.seed(123)
library("dplyr")
library(BVAR)
library(dplyr)
data <- read.delim("C:/Users/jesic/OneDrive/Desktop/Studies 2021/Second Semester/Time series/Project Dawie/Real/11_South Africa.txt")



# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.

```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}
Forecasting macroeconomic variables are critical for fiscal and monetary policy. One of the biggest challenges that economic agents face is having a clear view of the economy in real-time. Real gross domestic product (GDP) is only made available every quarter and is often heavily delayed\footnote{an example of this is that the 4th quarter GDP figures only become available eight weeks after the end of the quarter.}. This has severe consequences as it analyzes the economy challenging and therefore negatively affects policy decision making @kabundi.

Vector autoregressive (VAR) models have been extremely popular in economic literature for forecasting and analysing time-series variables. However, they are prone to problems of multicollinearity and over parameterization. To overcome these problems, @litter proposed a Bayesian method to analyse numerous parameters that would ensure that overfitting is not a problem. The Bayesian analysis relies on the specification of informative prior distributions. Over the years, the Bayesian vector autoregressive (BVAR) model has proven to be an effective tool to forecast key macroeconomic time series variables. This paper concludes that a BVAR model is more efficient than a VAR model in forecasting economic variables. 

This paper is organized as follows: Section 2 provides a literature review discussing past research and the different models that have been used to forecast economic growth.  Section 3 provides an overview of the Bayesian VAR framework. Section 4 discusses the data and section 5 provides a critical analysis of the estimation results. Section 6 discusses the results of several diagnostic tests. Finally, section 7 concludes. 

# Literature Review
Previous attempts to forecast the GDP of South Africa have been relatively successful, with Var-type models being the most common in economic literature. @chama uses a vector autoregressive (VAR) model to economic growth in South Africa. When the results were compared to a Minnesota prior Bayesian VAR, there were only small differences with the main forecasting results being very similar. However, @chama does explain that they only use three variables to forecast economic growth, which eliminates the over parameterization problems that are often accompanied by a large classical VAR. Similarly, @kabundi makes use of a VAR, Bayesian VAR (BVAR) and a Factor-Augmented VAR (FAVAR) to forecast economic growth for South Africa. For the Bayesian analysis, @kabundi uses Minnesota priors and treat the hyperparameters as additional parameters\footnote{This paper follows a similar approach}. They found that real GDP growth and inflation are the most important variables for forecasting economic growth. 

Even though VAR models have provided promising results, economic agents have also considered a variety of different models. @aron2002 argues against a VAR model to be used for forecasting and instead argues in favour of a multistep single equation forecasting model to predict GDP. When structural breaks are present, VAR models are prone to forecast errors by disregarding key macroeconomic variables. It has been proven that structural breaks are the main reason behind forecasting errors, yet VAR models do not account for this. It is important to account for a structural break when working with macroeconomic time series data due to South Africa experiencing periods of political crises, monetary policy regime shifts and financial liberalization. The multistep single equation forecasting model has proven to be simpler than a VAR analysis with reporting valid economic results. @aron2002 carefully analyse and test for structural breaks and found that changes in interest rates have a lesser effect on economic growth than before the shift in monetary policy occurred in the 1980s. With South Africa focusing more on trade openness from the 1990s onwards, the exchange rate seems to play an increasingly important role in predicting economic growth. 

@cepni uses a dynamic factor model (DFM) to forecast GDP in emerging markets. In this model, each macroeconomic variable is assigned a measure of importance that is based on the variable’s usage by market participants. Emerging market countries, like South Africa, often have data collection problems, however, the DFM mean square forecast error reduces as more data is added into the model. Therefore, the DFM adequately incorporates new information into the model. @cepni found that the dynamic factor model makes more accurate predictions than a benchmark linear model. 

However, Bayesian VAR models remains an increasingly beneficial method for forecasting. Imposing priors solve the over-fitting problem that one encounters when forecasting with a classical VAR. The bayesian method also allows the analyst to incorporate uncertainty into the model, which is extremely useful for forecasting. Therefore, this paper proposes a Bayesian VAR to forecast key economic variables 



# The Bayesian VAR Framework

Vector autoregressive (VAR) models became a popular econometric tool to analyse macroeconomic data after @sims1980 argued that current methods for econometric analysis are subject to overidentification. VAR models can successfully characterize any time series vector, without specifying numerous conditions. There are three types of VAR’s: a reduced form VAR, a recursive VAR, and a structural VAR. A reduced-form VAR is the most popular in economic literature, as it models each variable as a linear function of its past as well as the past values of all other variables included and includes a serially uncorrelated error term @stock2001. However, reduced VARS are usually not suitable for forecasting out-of-sample. To acquire a meaningful forecast, it is important to combine historic and prior information. Including prior information to estimate a VAR could lead to overfitting data especially if data is short, which would result in poor forecasting performance @can2011. Bayesian methods can solve the problem of modelling a VAR by making in-sample fettle less dramatic and improving forecasting performance. In this section, I discuss the Bayesian vector autoregressive (BVAR) approach I use to forecast GDP. 3A reduced form VAR(p) model can be denoted as follow,

\begin{equation}
Y_{t} = \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + … + \beta_p Y_{t-p} + \epsilon_t 
\end{equation}

where \(p\) specifies the number of lags and \(Y_t\) is a nx1 vector of endogenous variables at time \(t\), where n specifies the number of time series variables. \(\beta_0\) is an nx1 vector displaying the intercepts and \(\beta_{1}\)  to \(\beta_p\) are nx1 vectors  represents the coefficients.  \(\epsilon_t\) represents the error term and is assumed to be identically and independently distributed, with a mean of zero. To use the VAR model to forecast a variable, the model is simulated one period ahead.

\begin{equation}
\hat{Y_{t+1}} = \beta_0 + \beta_1 Y_{t} + \beta_2 Y_{t-1} + … + \beta_p Y_{t-p+1}
\end{equation}

A VAR is usually estimated by ordinary least square (OLS), as the parameters are consistent and normally distributed. However, estimating a VAR with Bayesian methods rather than OLS can result in more promising and reliable forecasts. The BVAR model uses Bayes Theorem, which is based on a prior, posterior and likelihood distribution, displayed in equation 3.3; 

\begin{equation}
\overbrace{P(\theta|y)}^{\text{Posterior}} = \frac{\overbrace{P(y|\theta)}^{\text{Likelihood}} \cdot \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{p(y)}_{\text{Normalizing constant}}}
\end{equation}

Where \(\theta\) is a random variable. Bayesian methods differ from OLS estimation focusing on general a posterior distribution, given the macroeconomic time series data and the prior distribution. Therefore, a Bayesian VAR provides a more accurate estimation of unknown parameters. 

The Minnesota prior developed by @litter is widely employed throughout economic literature and will also be used in the paper. @litter constructed his priors based on three facts of macroeconomic time series. The first fact is that most macroeconomic time series variables are characterized by a trend. The second fact is that the most recent past contains more important information than the distant past. The third fact being those past values of a macroeconomic time series variable contains more valuable information than the past value of any other variable @sacak. Based on these three facts, @litter constructed a prior distribution that becomes a multivariate random walk. The prior distribution that is centred around a random walk specification is given by; 

\begin{equation}
Y_{n,t} = \beta_0 + Y_{n,t-1} + \epsilon_{n,t}
\end{equation}

According to @car2010, the priors have three important characteristics:
\begin{enumerate}
\item The priors are noninformative for deterministic priors. 
\item The priors are independently and normally distributed for the lags of endogenous variables. 
\item The means of the prior distributions are set to zero except for the mean of the first lag of the dependent variable, which is then set to one. 
\end{enumerate}

The mean and variance of priors are characterized as follow:

\begin{equation}
\mathbb{E} \left[ (\beta_s)_{ij}|\Sigma  \right] =  
\begin{cases}   
1  \; \text{if i = j and s = 1} \\
0  \; \text{otherwise} 
\end{cases}
\end{equation}

\begin{equation}
cov\left[(\beta_s)_{ij}, (A_r)_{kl}|\Sigma \right] = 
\begin{cases}
\lambda^2\frac{1}{s^{\alpha}}\frac{\Sigma_{ik}}{\frac{\psi_j}{(d -M – 1)}} \; \text{if l=j and r =s} \\
0 \; \text{otherwise}
\end{cases}
\end{equation}

The most important hyperparameter is \(\lambda\), which controls the tightness of the prior and is set closer to zero for a tighter distribution. The larger \(\lambda\) is, the more likely the case is that the posterior distribution mirrors the sample information.  The hyperparameter \(\alpha\) controls the degree of shrinkage for distant observations. On variables other than the dependent variables, the hyperparameter \(\psi_j\) controls the prior’s standard deviation on lags. 

Furthermore, @2015prior explained that “additional priors can be implemented to reduce the importance of the deterministic component implied by VARs estimated conditioning on the initial observation”. An example of such a prior is the sum-of-coefficients (\emph{soc}), which incorporates the belief that economic variables can be represented by a process with cross-sectional linkages and unit-roots @litter. The \emph{soc} prior is implemented by adding a dummy-observation. The \emph{soc} prior is constructed as follow:

\begin{equation}
Y_{nxn}^{+} = diag\left(\frac{\bar{Y_0}}{\mu}\right)
\end{equation}

\begin{equation}
x_{nx(n+Mp)}^{+} = diag\left[0_{nx1},Y^+,…,Y^+\right]
\end{equation}

Where \(\bar{Y}\) is a n x 1 vector of average over the first p\footnote{note: p denotes the number of lag observations} observations of each variable. The hyperparameter \(\mu\) control the tightness of the prior.  Equation 3.8 creates the motive for the use of a dummy-initial-observation, als known as a single unit root (\emph{sor}) prior, which was designed to remove the \emph{soc} prior against cointegration, while ensure that overfitting doesn’t occur. The \emph{sor} is constructed as follow:

\begin{equation}
Y_{1xn}^{++} = diag\left(\frac{\bar{Y_0}^T}{\delta}\right)
\end{equation}

\begin{equation}
x_{1x(1+Mp)}^{++} = \left[\frac{1}{\delta},Y^{++}, …, Y^{++}\right]
\end{equation}

Where the hyperparameter \(\delta\) controls the tightness of the prior. 


# Data 

I propose a BVAR model estimated on seven-time series. The data was provided by @greenwood \footnote{details about the data collection and actual data series can be found here: http://qed.econ.queensu.ca/jae/datasets/greenwood-nimmo001/} and consist of 99 quarterly observations between 1980:Q3 and 2007:Q2. The data series used for observation is real gross domestic product (GDP), inflation, real stock price index, nominal exchange rate, real exports, real imports, and the oil price (UK Brent)\footnote{due to data restrictions, investment is not included.}. The variable selection implies that not all economic information is incorporated into the BVAR. However, the simple model does incorporate valuable economic variables. 
Inflation is included since it measures the average price changes in goods and services purchased by consumers and is included due to the significant negative relationship that it has with economic growth. By using the Granger causality test, @mamo found that inflation can be used to estimate economic growth. Real GDP is included since an increase in real GDP represents an increase in economic growth. @azmi found that, for developing countries, the exchange rate has a significant and positive relationship with economic growth. Therefore, South Africa should set out to maintain a high value of exchange rates. 
I have decided to include the real stock price index since stock prices have an extremely active role in the economy. Even though stock prices are much more volatile than the real economy, stock prices often have leading indicator properties for economic growth. This is because changes in the stock price are not necessarily driven by economic factors. Exports and imports also affect economic growth. @trem argued that an increase in economic growth and productivity is a result of an increase in exports and imports. South Africa should encourage export-and-import-led policies to increase economic growth. The last variable that I include is the oil price. It is well known that the oil price has a significant impact on economic growth and economic performance. An increase in the oil price harms national output, which then impacts spending and production and ultimately results in negative economic growth @nkomo. 

All variables, except inflation, are logged and first differenced to ensure stationarity.  The data is presented in figure \ref{das} below.


\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{repdata.jpg}
\caption{Represention of Data}
\label{das}
\end{figure}

```{r}
GDP <- diff(data$y)*100
xa <- data[-1,]
Inflation <- xa$dp
Stock_Price <- diff(data$q)*100
Exports <- diff(data$ex)*100
Imports <- diff(data$im)*100
Oil <- diff(data$po)*100
Exchange_Rate <- diff(data$e)*100


dats <- cbind(GDP, Inflation, Stock_Price ,Exports,Imports,Oil,Exchange_Rate)
dataa <- ts(dats, start=c(1980,3), frequency = 4)
# plot(dataa, main=" Representation of Data")

x <- as.data.frame(dataa)
```


# Empirical Analysis

In this section, I discuss the main results from the empirical analysis I performed. First, I discuss the process I followed setting up the priors and the model configuration. After, I assess the convergence of the Markov chain. Furthermore, the impulse response functions are analyzed as well as the forecasting results. 

## Configuration

The BVAR is specified with Minnesota priors. The hyperparameter \(\lambda\) has a Gamma distribution with a mode of 0.2 and a standard deviation of 0.4, as well as an upper and lower bound of 5 and 0.0001, respectively. The use of a gamma distribution is widely used as a conjugate prior in Bayesian analysis. Two dummy priors are also included: the sum of coefficients (\emph{soc}) and the single unit root (\emph{sur}) prior. Both dummy observations have a gamma distribution with a mode and a standard deviation equal to one. 
I chose to treat \(\lambda\), \emph{soc} and \emph{sur} as hierarchical priors. On the other hand, the hypermeters that are not treated hierarchically are treated as fixed and is equal to their mode (\(\alpha\) being an example of a fixed prior). Table \ref{hyp} shows the estimated values of the hyperparameters after optimization. 

\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 \(\lambda\) & \emph{soc} & \emph{sur}\\ 
 \hline
 0.12497 & 1.09338 & 0.17862 \\
 \hline
\end{tabular}
\caption{Hyperparamter Value after Estimation}
 \label{hyp}
\end{center}
\end{table}

## Assessing Markov Chain Convergence

The assessment of the convergence of the Markov Chain Monte Carlo (MCMC) algorithm is an important part to analyse the stability of the BVAR.  To explore the posterior hyperparameter space, the Metropolis-Hastings algorithm is used.  I specified 200 000 saved draws, of which the first 50 000 will be burned. Figure \ref{mcmc} displays the trace and density of the maximum likelihood (\emph{ml\) and the hierarchical hyperparameters (\(\lambda\).

A trace plot graphs the parameter value against the number of iterations. The trace plot informs you of whether the burn-in period is long enough. For the trace plot, one expects to see random scatter around the mean value meaning that the model has converged to its stationary distribution. If there is a trend in the sample space, then it implies that the parameter has not converged. The trace plot in figure \ref{mcmc} shows satisfactory results for \(\lambda\) and the maximum likelihood (\emph{ml}), implying that the chains are well mixed and that the chain most probably reached the correct distribution. The density plots use the kernel density estimate to display the probability function of the macroeconomic variables. As can be seen in figure \ref{mcmc}, 

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{traced.jpg}
\caption{Trace and density plot of the maximum likelihood and the hierarchical hyperparameters }
\label{mcmc}
\end{figure}

## Residual Plots 

Figure \ref{resid} displays the residual plots for all the macroeconomic variables. The residuals plots are used to ensure that the errors are independently and normally distributed. The residuals values are displayed on the y-axis and the fitted values on the x-axis. We would expect to see that the residuals are centred on zero, indicating that the model’s predictions are correct on average.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{resid.jpg}
\caption{Residual Plots}
\label{resid}
\end{figure}

\newpage

## Impulse response function 

Impulse response functions describe the progress of a variable along a specified time horizon after a specified shock has occurred. Figure \ref{irf} displays how GDP, Inflation and Oil respond to a shock of all seven macroeconomic variables described in section 4. 

A positive shock to GDP, also interpreted as a supply shock, has a positive effect on output causing prices to decrease.  A supply shock leads to an initial large increase in the supply of money, but also a decrease in purchasing power. Therefore, inflation increases. A supply shock can also be interpreted as a positive disruption of production which will raise production costs and lead to an increase in oil prices. As prices adjust, GDP and oil slowly return to their steady state, and the initial impact on inflation is long-lasting. 

A positive exchange rate shock has a positive and negative effects on the South African economy. An increase in the exchange rates means that exports are more expensive and decrease the price of imports. As a result, there will be a decrease in demand for domestic products from foreign markets leading to a decrease in GDP. However, the effect on GDP from a positive exchange rate shock cancels out after 15 periods as GDP returns to its steady-state.  A positive exchange rate shock will have a direct negative effect on inflation and a temporary negative effect on the oil price. 

An inflationary shock occurs when there is a sudden increase in the prices of commodities. When nominal interest rates are held constant, a positive inflationary shock will have a small negative effect on GDP.  This implies that an increase in nominal household wealth is less than the increase in the price level, therefore real wealth falls. Oil price and the level of inflation are often observed to move in the same direction. An inflationary shock leads to a large, short-lived, increase in the oil price, with the oil price returning to its steady-state after eight periods. 

Oil is an extremely important commodity that severely affects the global market. An oil shock can be challenging to analyse since the reason behind the shock is important.  A positive oil price shock decreases output and negatively affects spending and production patterns. Therefore, South Africa will experience a temporary decrease in GDP. Furthermore, an oil shock seems to have a small, relatively insignificant effect on inflation. The impact on GDP and inflation become smaller over time until both macroeconomic variables are back to their steady states. However, South Africa does have the opportunity to improve its net energy position which will lead to the country being less vulnerable to oil shocks\footnote{This was discussed at the recent COP26 conference}. 

Increasing trade has been extremely important for South Africa. A trade surplus contributes to economic growth. When the total amount of exports exceeds the total amount total of imports it implies a high level of output and an increase in productivity. An increase in imports is positively related to GDP, while exports are negatively related to it. This implies that the South African government should encourage trading of goods and services internally, with exporting goods not being the primary goal since it decreases GDP. A positive shock to imports and exports seem to have an insignificant initial impact on the oil price. 

A rising stock market usually means that the economy is growing and leads to greater investor confidence. The increase in wealth experienced by consumers leads to an increase in spending and purchasing power. However, since the stock market is volatile, the effect of a positive shock is usually short-lived, with GDP returning to its steady-state after eleven periods.  Since a positive shock to the stock market is accompanied by an increase in spending, it can create an increase in inflation if the demand for goods and services is considerably larger than the supply.  Furthermore, stock prices and oil prices tend to move in the same direction, therefore a positive shock to the stock market leads to a large, however short-lived, increase in the oil price.  

\begin{figure}[h]
\centering
\includegraphics[scale = 0.45, angle = 90]{use_shocks.jpg}
\caption{Impulse Response Functions}
\label{irf}
\end{figure}

## Forecasting 

To forecast the model, I split the data into a training and a testing set. Out of the 99 quarterly observations, the first 89 variables are put into the training dataset and the last 10 variables are put into the testing set. Therefore, the training set contains quarterly data from 1980:Q3 to 2004:Q4 and the testing set contains data from 2005:Q1 to 2007:Q2.  To analyse the BVAR, I compare the forecasting results with a VAR. The actual values of the data are displayed in table \ref{gdp10p} in the appendix. Figure \ref{123} displays the forecast results for the BVAR model for GDP, inflation and the oil price and figure \ref{1234} displays the forecasting results for the VAR model. 

\begin{figure}[h]
\centering
\includegraphics[scale = 0.6]{for1.jpg}
\caption{BVAR forecasting results}
\label{123}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.8]{forcast.jpg}
\caption{VAR forecasting results}
\label{1234}
\end{figure}

Table \ref{gdpstat} displays statistic results to estimate and compare the accuracy of the two models. The Mean Square Error (\emph{MSE}) is the average square of the difference between the predicted values and the actual values. The BVAR model produces smaller statistical results than the VAR model, indicating that the BVAR produces more efficient results. The Root Mean Square Error (\emph{RMSE}) calculates the standard deviations of the residuals. It provides information about the performance of the model by comparing the predicted value to the actual value. The smaller the value, the better the model’s performance. The \emph{RMSE} statistic indicates that the BVAR model performs better than the BAR model. The Mean Absolute Error (\emph{MAE}) computes the average absolute difference between the predicted values and the observed values. It measures the accuracy for continuous-time series variables. For forecasting, you will want the \emph{MAE} to be as low as possible. Therefore, the \emph{MAE} statistic shows that the BVAR model provides a much more accurate forecast than the VAR model for all variables.  It is important to note that the \emph{MAE} statistic for the oil price is extremely high for both models, suggesting that other models should be explored if economic agents want to forecast the oil price. 

There are two Theil-U statistics, the first measurement (\emph{TheilU:1}) measures the forecast accuracy. The second measurement (\emph{TheilU:2}) measures the forecast quality. The BVAR model has a TheilU statistics of less than one for all three variables. This means that the BVAR forecast is better than guessing.  However, the TheilU statistics for the VAR model is more than one, suggesting that the model makes very poor predictions. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|} 
  \hline
 Statistic & BVAR & VAR \\ 
 \hline
 \multicolumn{3}{|c|}{GDP} \\
 \hline
MSE & 0.1892822 & 1.745656 \\
RMSE & 0.4350658 & 1.321233 \\
MAE & 0.3997396 & 1.213998 \\
TheilU:1 & 0.1761636 & 0.423099 \\
TheilU:2 & 0.3439547 & 1.044541 \\
\hline
\multicolumn{3}{|c|}{Inflation} \\
 \hline
 MSE & 2.706111e-05 & 0.002002638 \\
RMSE & 0.005202029 & 0.04475084 \\
MAE & 0.003930009 & 0.04227154 \\
TheilU:1 & 0.2885939 & 0.9826374 \\
TheilU:2 & 0.4883881 & 4.201395 \\
\hline
\multicolumn{3}{|c|}{Oil Price} \\
 \hline
MSE & 87.49288 & 1591.65 \\
RMSE & 9.353763 & 39.89549 \\
MAE & 8.182195 & 34.72016 \\
TheilU:1 & 0.6246275 & 0.8361373 \\
TheilU:2 & 0.8281177 & 3.532072 \\
\hline
\end{tabular}
\caption{Forecasting Statistics }
 \label{gdpstat}
\end{center}
\end{table}



```{r}

## BVAR 


# mn <- bv_minnesota(lambda = bv_lambda(mode = 0.2, sd = 0.4, min = 0.0001, max = 5),alpha = bv_alpha(mode = 2), var = 1e07)

# soc <- bv_soc(mode = 1, sd = 1, min = 1e-04, max = 50)
# sur <- bv_sur(mode = 1, sd = 1, min = 1e-04, max = 50)

# priors <- bv_priors(hyper = "auto", mn = mn, soc = soc, sur = sur)

# mh <- bv_metropolis(scale_hess = c(0.05, 0.0001, 0.0001),adjust_acc = TRUE, acc_lower = 0.25, acc_upper = 0.45)

#la <- bvar(x, lags=1)

# run <- bvar(x, lags = 10, n_draw = 50000, n_burn = 25000, n_thin = 1, priors = priors, mh = mh, verbose = TRUE)


# print(run)

#plot(run)
#plot(run, type = "dens", vars_response = "Oil", vars_impulse = "Oil-lag1")

# fitted(run, type = "mean")

# plot(residuals(run, type = "mean"), vars = c("Oil", "Exchange_Rate"))

# opt_irf <- bv_irf(horizon = 16, identification = TRUE)

# irf(run) <- irf(run, opt_irf, conf_bands = c(0.05, 0.16))

# plot(irf(run), area = TRUE, vars_impulse = c("Oil", "Exchange_Rate"), vars_response = c(1:2, 6))

# predict(run) <- predict(run, horizon = 16, conf_bands = c(0.05, 0.16))
# plot(predict(run), area = TRUE, t_back = 32, vars = c("y", "dp", "p"))

```
 
# Diagnostics 
In this section, I will preform numerous diagnostic and robustness test to determine whether the validity and robustness of the BVAR that was forecasted in section five. I test for stationary and the I analysize the optimal number of lags. 

## Stationarity 

Before preceding to analyse macroeconomic variables, it is important to ensure that the variables are stationary. Having non-stationary time series data can lead to spurious results implying that the stationarity of macroeconomic time series data can significantly affect forecasting behaviour @van. I log and first difference all variables, except inflation, to ensure stationarity. I perform three robustness checks to confirm stationarity. First, I plot the autocorrelation function (ACF) for each macroeconomic variable. The ACF, displayed in figure \ref{autoc} describes the relationship between the average data points and the preceding data points. On the x-axis, you will have the number of lags and on the y-axis, you will have your autocorrelation value. For all variables, except inflation, we notice there is a significant spike at lag one and much lower subsequent lags. Inflation also has a significant spike at lag one, followed by slightly lower, and to some degree constant spikes. Except for inflation, all variables degrade to zero rapidly, confirming stationarity. Figure /ref{autoc} implies that inflation is non-stationary and should be further investigated.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{station.jpg}
\caption{Autocorrelation Function Analysis}
\label{autoc}
\end{figure}

To further investigate the stationarity of the macroeconomic variables, I perform the Augmented Dickey-Fuller (ADF) test, with no trend since I detrended the variables. The results are displayed in table \ref{adf}. The null hypothesis states that a unit root is present in the time series data and the alternative hypothesis states that the data is stationary. The results show a p-value of less than 0.05 for all variables, implying that we can reject the null hypothesis. Thereby, an indication that the series is stationary

\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Variable & ADF statistic & P-Value \\ 
 \hline
 GDP & -5.0533 & 0.01 \\ 
 Inflation & -3.7828 & 0.02224 \\
 Stock Price & -4.9269 & 0.01 \\
 Exports & -3.9242 & 0.01538 \\
 Imports & -4.7132 & 0.01 \\
 Oil & -6.4081 & 0.01 \\
 Exchange Rate & -4.4494 & 0.01 \\
 \hline
\end{tabular}
\caption{Augmented Dickey-Fuller Test Analysis}
 \label{adf}
\end{center}
\end{table}

The ADF test results have a high probability of a type I error, meaning that they can falsely reject the null hypothesis. Therefore, it needs to be interpreted with caution. To ensure the validity of the results, I preform the Phillips-Perron (PP) test.

The PP test also tests whether a variable has a unit root.  The advantage of analysing the PP test is that it is non-parametric, meaning that it does not require selecting the level of serial correlation as the ADR does. The null hypothesis is that the variables contain a unit root, and the alternative hypothesis is that the variable was generated by a stationary process. The results in table \ref{pptest} indicate that all macroeconomic variables are stationary. 


\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Variable & PP statistic & P-Value \\ 
 \hline
 GDP & -53.825 & 0.01 \\ 
 Inflation & -61.525 & 0.01 \\
 Stock Price & -82.78 & 0.01 \\
 Exports & -122.93 & 0.01 \\
 Imports & -114.63 & 0.01 \\
 Oil & -80.848 & 0.01 \\
 Exchange Rate & -98.411 & 0.01 \\
 \hline
\end{tabular}
\caption{Phillips-Perron unit Root Test Analysis}
 \label{pptest}
\end{center}
\end{table}

 
## Optimal lag length

Determining the lag length for a Bayesian VAR is an extremely important econometric exercise. I considered four unique criteria to determine the optimal autoregressive lag length. Among the criteria that are considered is the Akaike Information Criterion (AIC), the Schwarz Criterion (SC) also known as the Bayesian Information Criterion (BIC), the Hannan Quinn (HQ) and the Final Prediction Error (FPE).  @garnitz found that model selection by the AIC or SC leads to very parsimonious models in the majority of cases for forecasting GDP growth. @liew found that the AIC and FPE produce superior results over other criteria. However, this study will follow the recommendation of @gupta and @clark and use the AIC lag criteria to choose the optimal lag length.

\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 AIC(n) & HQ(n) & SC(n) & FPE(n) \\ 
 \hline
 10 & 1 & 1 & 1\\ 
 \hline
\end{tabular}
\caption{lag length analysis}
 \label{lag}
\end{center}
\end{table}

 
# Conclusion

It is crucial for monetary and fiscal policy makers to be able to make accurate predictions about economic growth. Bayesian statistics have become a popular tool to forecast important macroeconomic variables. In this paper, I estimate a seven variable BVAR model and compare the forecasting results with a VAR model. The diagnostic results show that all variables are stationary and that a model with 10 lags is most appropriate. The BVAR model is estimated with Minnesota priors and two dummy priors. The empirical results validate that BVAR has an efficient level of forecast accuracy and quality, and it preferred over the VAR. Therefore, I conclude that a BVAR model can be highly useful for policy analysis. 

\newpage

# References {-}

<div id="refs"></div>


# Appendix {-}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|} 
 \hline
 Date & Actual & Forecast BVAR & Forecast VAR\\ 
 \hline
  \multicolumn{4}{|c|}{GDP} \\
 \hline
2005Q1 & 1.3641975 & 1.7448787 & - 0.8908903 \\
2005Q2 & 1.2130095 & 1.7434149 & 0.1065763 \\
2005Q3 & 0.9688178 & 1.5251118 & -0.3492994 \\
2005Q4 & 1.4387614 & 1.2852046 & 0.4464542 \\
2006Q1 & 1.4957687 & 1.1148588 & 1.5528151 \\
2006Q2 & 1.1248029 & 0.8768851 & 2.1273837 \\
2006Q3 & 1.5373958 & 0.8314630 & 2.6274903 \\
2006Q4 & 1.3361993 & 0.8156417 & 2.6859277 \\
2007Q1 & 0.8991781 & 0.7431564 & 2.3765362 \\
2007Q2 & 1.0947265 & 0.7296073 & 2.5859514 \\
 \hline
\multicolumn{4}{|c|}{Inflation} \\
\hline
2005Q1 & 0.006368598 & 0.004051628 & -0.01265742 \\
2005Q2 & 0.005283041 & 0.005142886 & -0.01726220 \\
2005Q3 & 0.006773210 & 0.008334394 & -0.03529701 \\
2005Q4 & 0.002397284 & 0.007580780 & -0.03532820 \\
2006Q1 & 0.005726190 & 0.007872427 & -0.02663917 \\
2006Q2 & 0.009536616 & 0.008883043 & -0.03608311 \\
2006Q3 & 0.018747035 & 0.007656673 & -0.02982121\\
2006Q4 & 0.010118324 & 0.006849587 & -0.03457038 \\
2007Q1 & 0.011258538 & 0.007385887 & -0.05109997 \\
2007Q2 & 0.017610026 & 0.008543302 & -0.05013784 \\
\hline
\multicolumn{4}{|c|}{Oil Price} \\
\hline
2005Q1 & 8.0106647 & 3.83536202 & 9.715215 \\
2005Q2 & 17.6070462 & 8.95983541 & -5.681085\\
2005Q3 & -7.7909729 & -0.01693524 & -31.130308 \\
2005Q4 & 8.3794895 & 0.41643603 & -34.810443 \\
2006Q1 & 12.0328698 & 5.41052479 & -8.030314 \\
2006Q2 & 0.3763495 & 0.85631826 & -33.676059 \\
2006Q3 & -16.0104983 & 0.23662992 & -48.439894 \\
2006Q4 & -2.8129879 & 2.94611692 & -35.927783 \\
2007Q1 & 16.8641448 & 1.18515221 & -51.381779 \\
2007Q2 & 8.7831482 & 0.30833741 & -58.990796 \\
\hline 
\end{tabular}
\caption{10 period ahead forecast results }
 \label{gdp10p}
\end{center}
\end{table}


